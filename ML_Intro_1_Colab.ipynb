{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tfoesch/awesome-python/blob/master/ML_Intro_1_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIFhlpwiju-u"
      },
      "source": [
        "# Example 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jg3h7CVq3zX"
      },
      "source": [
        "If you want to save everything: `Click on File -> Save a Copy in Drive`\n",
        "\n",
        "Now you have it in your drive and can always get back to it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9CdCEaplwg7"
      },
      "source": [
        "## Step -1: Download Dataset\n",
        "\n",
        "We download the following dataset: https://github.com/eth-student-project-house/ws-ml-intro-1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eFGh5vqjpi_"
      },
      "outputs": [],
      "source": [
        "# The ! at the beginning is a jupyter notebook specific. It tells the server to execute this as\n",
        "# shell (instead of python) code.\n",
        "# wget: program to download\n",
        "# unzip: program to unzip (-o tells to overwrite if files exist)\n",
        "!wget https://github.com/eth-student-project-house/ws-ml-intro-1/archive/refs/heads/main.zip\n",
        "!unzip -o /content/main.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMam3oDanYs3"
      },
      "source": [
        "On the left hand side, you can click on the folder icon and browse all files in this runtime. When you double click a file, you can see a preview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSdE0xl6nhaE"
      },
      "source": [
        "## Step 0: Create Embeddings\n",
        "\n",
        "Download a vector database: chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_u0bxqi1nlZ7"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb\n",
        "# It finishes mit `Successfully installed backoff-2.2.1 chromadb-0.3.25...` \n",
        "# You might see an error message just before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjlwsqaMgC8r"
      },
      "source": [
        "1. Initialize the database and create what they call a collection. We tell it to use it's own embedding model. (We could use openai embeddings)\n",
        "\n",
        "2. Then, load all data into this collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JLun1ghlpZQX"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "import os\n",
        "\n",
        "from chromadb.config import Settings\n",
        "from chromadb.utils  import embedding_functions\n",
        "\n",
        "client = chromadb.Client(Settings(\n",
        "  chroma_db_impl=\"duckdb+parquet\",\n",
        "  persist_directory=\"/content/.chromadb\" # Optional, defaults to .chromadb/ in the current directory\n",
        "))\n",
        "\n",
        "# embedding function\n",
        "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
        "\n",
        "collection = client.create_collection(name=\"ml-intro-1\", embedding_function=default_ef)\n",
        "print(\"created new collection\")\n",
        "\n",
        "dataDirectory = \"/content/ws-ml-intro-1-main/data/base\"\n",
        "\n",
        "for filename in os.listdir(dataDirectory):\n",
        "  if filename.endswith('.md'):\n",
        "    # Open the file and read its contents\n",
        "    filepath = os.path.join(dataDirectory, filename)\n",
        "    with open(filepath, 'r') as f:\n",
        "        fileContent = f.read()\n",
        "    \n",
        "    # Call your function on the file content\n",
        "    print(f\"adding document {filepath}\")\n",
        "    collection.add(documents=[fileContent], ids=[filepath])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEGbsWWC0Jc6"
      },
      "source": [
        "Let's have a look into our database and their vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XiJGMi0Qzd8c"
      },
      "outputs": [],
      "source": [
        "collection.peek(limit=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BpKRjpXZgGe"
      },
      "source": [
        "## Step 1+2: Sample Questions\n",
        "\n",
        "- Are there any green projects?\n",
        "- Are there any green projects? If so, please summarize their similarities.\n",
        "- Are there any green projects? If so, please summarize their similarities. Output it in form of a list\n",
        "- What is the Makerspace?\n",
        "- What is the Ideaspace?\n",
        "- What is the Digital Makerspace?\n",
        "- What is the SPH?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZgfgQ5lhzoue"
      },
      "outputs": [],
      "source": [
        "#### Step 1: Question\n",
        "question = \"Are there any green projects?\" #@param {type:\"string\"}\n",
        "nr_of_results = 2 #@param {type:\"slider\", min:1, max:10}\n",
        "#### Step 2: Search Vector Database\n",
        "results = collection.query(query_texts=[question], n_results=nr_of_results)\n",
        "# line below just outputs\n",
        "results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-r879_TltIf"
      },
      "source": [
        "## Step 3 - Use the Language Model (FINALLY)\n",
        "\n",
        "Now we are ready to create the final prompt.\n",
        "\n",
        "First install the libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E96LWGZG1yms"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nnbZ45fscaH"
      },
      "source": [
        "Since we are using \"Open\"AI we need an API Key (for billing). This key is provided by SPH."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PdQIQxrk-YZ-"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "OPENAI_API_KEY = \"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\"\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STpxYrgrsjxB"
      },
      "source": [
        "We Tell the model how to act. We use the \"chat\" function.\n",
        "\n",
        "- We first give it a system prompt. This tells the Language Model how to act\n",
        "- And finally, we construct *our* question and give it some context.\n",
        "- Later, we might want to add an example, so that the assistant responds in the way we want it to.\n",
        "\n",
        "Basically you have a lot of freedom how to create these prompts and examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e-akEAtZ0vbC"
      },
      "outputs": [],
      "source": [
        "### Now tell the language model how to act. This is an example. (it might be a rather dumm instruction given our context!)\n",
        "\n",
        "# Behaviour\n",
        "system = f\"\"\"\\\n",
        "You are a helpful assistant. \n",
        "\"\"\"\n",
        "\n",
        "# Question + Context\n",
        "### This is where we add our question â¬‡â¬‡\n",
        "### TODO: You might need to properly print the array\n",
        "user2 = f\"\"\"\\\n",
        "CONTEXT: {results['documents'][0]}\n",
        "\n",
        "QUESTION: {question}\n",
        "\"\"\"\n",
        "\n",
        "### This is the final prompt\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": system},\n",
        "    {\"role\": \"user\", \"content\": user2},\n",
        "]\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0hUHSEb84WsL"
      },
      "outputs": [],
      "source": [
        "### Step 3: Call to OPEN AI\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    temperature=0.6,\n",
        "    max_tokens=800,\n",
        "    top_p=1,\n",
        "    frequency_penalty=1,\n",
        "    presence_penalty=1\n",
        ")\n",
        "\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_0_eDmQbemG"
      },
      "source": [
        "Now let's see how the Language Model replies, when we give it a more concise construction set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dQMED3HRbkXO"
      },
      "outputs": [],
      "source": [
        "# source: https://github.com/gannonh/chatgpt-pgvector\n",
        "### Now tell the language model how to act. This is an example. (it might be a rather dumm instruction given our context!)\n",
        "\n",
        "# Behaviour\n",
        "system = f\"\"\"\\\n",
        "You are a helpful assistant. When given CONTEXT you answer questions using only that information,\\\n",
        "and you always format your output in markdown. You include code snippets if relevant. If you are unsure and the answer\\\n",
        "is not explicitly written in the CONTEXT provided, you say\\\n",
        "\"Sorry, I don't know how to help with that.\"  If the CONTEXT includes \\\n",
        "source URLs include them under a SOURCES heading at the end of your response. Always include all of the relevant source urls \\\n",
        "from the CONTEXT, but never list a URL more than once (ignore trailing forward slashes when comparing for uniqueness). Never include URLs that are not in the CONTEXT sections. Never make up URLs\\\n",
        "\"\"\"\n",
        "\n",
        "# Example\n",
        "user1 = f\"\"\"\\\n",
        "CONTEXT:\n",
        "Next.js is a React framework for creating production-ready web applications. It provides a variety of methods for fetching data, a built-in router, and a Next.js Compiler for transforming and minifying JavaScript code. It also includes a built-in Image Component and Automatic Image Optimization for resizing, optimizing, and serving images in modern formats.\n",
        "SOURCE: nextjs.org/docs/faq\n",
        "\n",
        "QUESTION: \n",
        "what is nextjs? \n",
        "\"\"\"\n",
        "\n",
        "assistant1 = f\"\"\"\\\n",
        "Next.js is a framework for building production-ready web applications using React. It offers various data fetching options, comes equipped with an integrated router, and features a Next.js compiler for transforming and minifying JavaScript. Additionally, it has an inbuilt Image Component and Automatic Image Optimization that helps resize, optimize, and deliver images in modern formats.\n",
        "  \n",
        "\\`\\`\\`js\n",
        "function HomePage() {{\n",
        "  return <div>Welcome to Next.js!</div>\n",
        "}}\n",
        "\n",
        "export default HomePage\n",
        "\\`\\`\\`\n",
        "\n",
        "SOURCES:\n",
        "https://nextjs.org/docs/faq\n",
        "\"\"\"\n",
        "\n",
        "# Question + Context\n",
        "### This is where we add our question â¬‡â¬‡\n",
        "### TODO: You might need to correct the array reference! (right now, only one result is taken into account)\n",
        "user2 = f\"\"\"\\\n",
        "CONTEXT: {results['documents'][0]}\n",
        "\n",
        "QUESTION: {question}\n",
        "\"\"\"\n",
        "\n",
        "### This is the final prompt\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": system},\n",
        "    {\"role\": \"user\", \"content\": user1},\n",
        "    {\"role\": \"assistant\", \"content\": assistant1},\n",
        "    {\"role\": \"user\", \"content\": user2},\n",
        "]\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc8HhCVNbnXf"
      },
      "outputs": [],
      "source": [
        "### Step 3: Call to OPEN AI\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    temperature=0.6,\n",
        "    max_tokens=2100,\n",
        "    top_p=1,\n",
        "    frequency_penalty=1,\n",
        "    presence_penalty=1\n",
        ")\n",
        "\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8ZIjQeRjKOF"
      },
      "source": [
        "## Next steps: Play with it ðŸš€ðŸ’­\n",
        "\n",
        "- In Step 3: Think about the usage of `results['documents'][0]` for the CONTEXT.\n",
        "- In Step 3: Use \"gpt4\" instead of \"gpt-3.5-turbo\" in \"Step 3\"\n",
        "- In Step 2: Use more results from database `nr_of_results=4`?\n",
        "- In Step 3: Play with temperature, max_tokens\n",
        "- In Step 0: Look at the dataset. There is another dataset called: `with_source`. This dataset has the link to the document inside: `SOURCE: https:/....` When using this dataset, you can actually output the source to your document.\n",
        "- In Step 3: Change the systemprompt: `You are a very euphoric...`\n",
        "- In Step 3: Change the example\n",
        "- Generally: Be aware, whenever you change something, you might want to compare how the new answer is compared to the old one? How could you do that reliably?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnEuKQtoaRRm"
      },
      "source": [
        "\n",
        "# Example 2 (for yourself)\n",
        "\n",
        "There is another data set, the project hub: Download it, add it to a new collection. Or use your own data set export. You can use the script as is, as long as all files are markdown and are in one folder.\n",
        "\n",
        "`https://n.ethz.ch/~thfrei/download/ml-intro-1-sph-projecthub.zip`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fXFlqV88cR6C"
      },
      "outputs": [],
      "source": [
        "!wget https://n.ethz.ch/~thfrei/download/ml-intro-1-sph-projecthub.zip\n",
        "# unzip will extract into /content/notion\n",
        "!unzip -o /content/ml-intro-1-sph-projecthub.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GutEHax-alf4"
      },
      "source": [
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ". \n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Axo5F1tcMOQ"
      },
      "source": [
        "# Appendix / Snippets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amt-6rCjQFAC"
      },
      "source": [
        "### Format as Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLvEIxLW2FNs"
      },
      "outputs": [],
      "source": [
        "# This let's you format the output nicely!\n",
        "from IPython.display import display, Markdown\n",
        "display(Markdown(response.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XCo73GsQHuh"
      },
      "source": [
        "### Check token limit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B875X4LER7CZ"
      },
      "source": [
        "You can also use the hosted version: https://platform.openai.com/tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jH3hZxD35qSs"
      },
      "outputs": [],
      "source": [
        "### Check token limit.\n",
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo\"):\n",
        "    # source: learn.microsoft.com + modification\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":  # if there's a name, the role is omitted\n",
        "                num_tokens += -1  # role is always required and always 1 token   \n",
        "    num_tokens += 3\n",
        "    # every reply is primed with <|start|>assistant<|message|> # modification\n",
        "    return num_tokens\n",
        "\n",
        "print(f\"{num_tokens_from_messages(messages)} number of tokens to be sent in our request\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}